{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viswambhar-yasa/image_captioning/blob/master/training_reward_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCaXLacBGeyg",
        "outputId": "fe2f4d8b-3044-4966-8f75-d1fa58e1c6c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'image_captioning'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 22 (delta 8), reused 15 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (22/22), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/viswambhar-yasa/image_captioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VsGCW4z_Gi35"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "\n",
        "\n",
        "def downloading_extraction(link, extraction_path='.'):\n",
        "    url = urlopen(link)\n",
        "    zipfile = ZipFile(BytesIO(url.read()))\n",
        "    zipfile.extractall(path=extraction_path)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    images_link = 'https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip'\n",
        "    downloading_extraction(images_link)\n",
        "    text_link = \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\"\n",
        "    downloading_extraction(text_link)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Embedding, LSTM, BatchNormalization, Bidirectional\n",
        "from tensorflow.keras.applications import Xception, InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.python.keras.layers.recurrent import GRU\n",
        "\n",
        "\n",
        "def image_encoder(img_input, trainable_layers=0, CNN_Type='Xception', Embed_Size=256, display=False):\n",
        "    print('Building CNN model')\n",
        "    if CNN_Type == 'Xception':\n",
        "        cnn_pre_trained_model = Xception(include_top=False, weights='imagenet', input_tensor=img_input)\n",
        "    else:\n",
        "        cnn_pre_trained_model = InceptionV3(include_top=False, weights='imagenet', input_tensor=img_input)\n",
        "    for i, layer in enumerate(cnn_pre_trained_model.layers):\n",
        "        if len(cnn_pre_trained_model.layers) - i < trainable_layers:\n",
        "            layer.trainable = True\n",
        "        else:\n",
        "            layer.trainable = False\n",
        "    cnn_inputs = cnn_pre_trained_model.inputs\n",
        "    base_model = cnn_pre_trained_model.output\n",
        "    base_model = GlobalAveragePooling2D(name='global_average_pooling')(base_model)\n",
        "    embed_image = tf.keras.layers.Dense(Embed_Size, activation='tanh', name='embed_image')(base_model)\n",
        "    feature_extraction_model = Model(inputs=cnn_inputs, outputs=embed_image, name='CNN encoder model')\n",
        "    print('CNN model {output shape}:', embed_image.shape)\n",
        "    if display:\n",
        "        tf.keras.utils.plot_model(feature_extraction_model, to_file='base_model.png', show_shapes=True)\n",
        "    return feature_extraction_model\n",
        "\n",
        "\n",
        "def txt_decoder(rnn_input, Embed_Size=256, Bi_Direction=False, RNN_Type='LSTM', RNN_Layers=2):\n",
        "    print('Building RNN model')\n",
        "    for i in range(RNN_Layers):\n",
        "        x = BatchNormalization()(rnn_input)\n",
        "        if RNN_Type == 'LSTM':\n",
        "            if i == (RNN_Layers - 1):\n",
        "                if Bi_Direction:\n",
        "                    rnn_out = Bidirectional(LSTM(int(Embed_Size/2)))(x)\n",
        "                else:\n",
        "                    rnn_out = LSTM(Embed_Size)(x)\n",
        "            else:\n",
        "                if Bi_Direction:\n",
        "                    rnn_out = Bidirectional(LSTM(int(Embed_Size/2), return_sequences=True))(x)\n",
        "                else:\n",
        "                    rnn_out = LSTM(Embed_Size, return_sequences=True)(x)\n",
        "        else:\n",
        "            if i == (RNN_Layers - 1):\n",
        "                if Bi_Direction:\n",
        "                    rnn_out = Bidirectional(GRU(Embed_Size))(x)\n",
        "                else:\n",
        "                    rnn_out = GRU(Embed_Size)(x)\n",
        "            else:\n",
        "                if Bi_Direction:\n",
        "                    rnn_out = Bidirectional(GRU(Embed_Size/2, return_sequences=True))(x)\n",
        "                else:\n",
        "                    rnn_out = GRU(Embed_Size, return_sequences=True)(x)\n",
        "        rnn_input = rnn_out\n",
        "    return rnn_out\n",
        "\n",
        "\n",
        "def Caption_model_gen(NET, img_shape=(256, 256, 3), vocab_size=8763, Embed_Size=512, max_length=20, display=False):\n",
        "    img_input = tf.keras.Input(shape=img_shape)\n",
        "    cnn_model = image_encoder(img_input, trainable_layers=0, CNN_Type='InceptionV3', display=False)\n",
        "    embed_image = tf.keras.layers.Dense(Embed_Size, activation='tanh')(cnn_model.output)\n",
        "\n",
        "    text_input = tf.keras.Input(shape=(max_length,))\n",
        "    Embedding_layer = Embedding(input_dim=vocab_size, output_dim=Embed_Size, input_length=max_length, mask_zero=True)(\n",
        "        text_input)\n",
        "\n",
        "    whole_seq_output = txt_decoder(Embedding_layer, Embed_Size=Embed_Size,\n",
        "                                                                          Bi_Direction=False, RNN_Type='LSTM',\n",
        "                                                                          RNN_Layers=3)\n",
        "    print('final_carry_state {rnn output shape}:', whole_seq_output.shape)\n",
        "    rnn_output = whole_seq_output\n",
        "    if NET == 'policy':\n",
        "        image_txt_embed = tf.keras.layers.add([embed_image, rnn_output])\n",
        "        print('Image and text {add shape}:', image_txt_embed.shape)\n",
        "        policy_net_output = tf.keras.layers.Dense(vocab_size, activation='softmax')(image_txt_embed)\n",
        "        policy_net_model = Model(inputs=[img_input, text_input], outputs=policy_net_output, name='Policy_Net')\n",
        "\n",
        "        print('output {shape}', policy_net_output.shape)\n",
        "        print('Policy Net built successfully \\n')\n",
        "        if display:\n",
        "            tf.keras.utils.plot_model(policy_net_model, to_file='policy_net.png', show_shapes=True)\n",
        "        return policy_net_model\n",
        "    elif NET == 'value':\n",
        "        image_txt_embed = tf.keras.layers.concatenate([embed_image, rnn_output], axis=-1)\n",
        "        print('Image and text {concat shape}:', image_txt_embed.shape)\n",
        "        hidden_layer_1 = Dense(1024, activation='tanh', name='MLP_layer1')(image_txt_embed)\n",
        "        hidden_layer_2 = Dense(512, activation='tanh', name=\"MLP_layer2\")(hidden_layer_1)\n",
        "        value_net_outputs = Dense(1, activation='tanh', name='decoder_output')(hidden_layer_2)\n",
        "        value_net_model = Model(inputs=[img_input, text_input], outputs=value_net_outputs, name='Value_Net')\n",
        "        print('output {shape}', value_net_outputs.shape)\n",
        "        print('Value Net built successfully \\n')\n",
        "        if display:\n",
        "            tf.keras.utils.plot_model(value_net_model, to_file='value_net.png', show_shapes=True)\n",
        "        return value_net_model\n",
        "    else:\n",
        "        feature_vector = Dense(512, activation='tanh')(embed_image)\n",
        "        text_sequence_vector = Dense(512, activation='tanh', name='rnn_linear')(rnn_output)\n",
        "        print('Image feature vector shape:', feature_vector.shape)\n",
        "        print('Text sequence vector shape:', text_sequence_vector.shape)\n",
        "        reward_model = Model(inputs=[img_input, text_input], outputs=[feature_vector, text_sequence_vector],\n",
        "                             name='reward_net_model')\n",
        "        print('Reward Net built successfully \\n')\n",
        "        if display:\n",
        "            tf.keras.utils.plot_model(reward_model, to_file='reward_net.png', show_shapes=True)\n",
        "        return reward_model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print('TensorFlow Version', tf.__version__)\n",
        "    #actor_model = Caption_model_gen('policy')\n",
        "    #critic_model = Caption_model_gen('value')\n",
        "    #reward = Caption_model_gen('reward')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "907XvN7wzXz3",
        "outputId": "ac3cacae-671d-4eb2-98a7-9455776b0bd6"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version 2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2fEKMu1e4XxR"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/image_captioning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDfI0dk_6FSr",
        "outputId": "c1f0481f-8afc-46b7-fde1-f282a81aa5b6"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/image_captioning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class data_processing:\n",
        "    def __init__(self, text_file_path):\n",
        "        self.text_file_path = text_file_path\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def extraction_captions(self, images_id_text):\n",
        "        description_map = dict()\n",
        "        text = open(self.text_file_path, 'r', encoding='utf-8').read()\n",
        "        images = open(images_id_text, 'r', encoding='utf-8').read()\n",
        "        img_dic = []\n",
        "        for img_id in images.split('\\n'):\n",
        "            img_dic.append(img_id)\n",
        "        for lines in text.split('\\n'):\n",
        "            line_split = lines.split('\\t')\n",
        "            if line_split == ['']:\n",
        "                continue\n",
        "            image_id = line_split[0][:-2]\n",
        "            image_des = line_split[1]\n",
        "            if image_id in img_dic:\n",
        "                if image_id not in description_map:\n",
        "                    description_map[image_id] = list()\n",
        "                description_map[image_id].append(image_des)\n",
        "        return description_map\n",
        "\n",
        "    def cleaning_sequencing_captions(self, images_id_text):\n",
        "        captions_dic = self.extraction_captions(images_id_text)\n",
        "        caption_list = []\n",
        "        for img_id, des_list in captions_dic.items():\n",
        "            for i in range(len(des_list)):\n",
        "                caption = des_list[i]\n",
        "                caption = ''.join(caption)\n",
        "                caption = caption.split(' ')\n",
        "                caption = [word.lower() for word in caption if len(word) > 1 and word.isalpha()]\n",
        "                caption = ' '.join(caption)\n",
        "                des_list[i] = 'startseq ' + caption + ' endseq'\n",
        "                caption_list.append('startseq ' + caption + ' endseq')\n",
        "        max_length = max(len(des.split()) for des in caption_list)\n",
        "        print('max_length of captions', max_length)\n",
        "        return caption_list,captions_dic\n",
        "\n",
        "    def tokenization(self, captions_for_token, num_wrds=5000) -> None:\n",
        "        tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_wrds, oov_token='<unknw>')\n",
        "        tokenizer.fit_on_texts(captions_for_token)\n",
        "        self.tokenizer = tokenizer\n",
        "        pass\n",
        "\n",
        "    def sentence_tokenizing(self, captions_dic) -> dict:\n",
        "        token_cap_dic = dict()\n",
        "        print('Vocab size', self.tokenizer.num_words)\n",
        "        for img_id, des_list in captions_dic.items():\n",
        "            for i in range(len(des_list)):\n",
        "                caption = des_list[i]\n",
        "                cap_token = self.tokenizer.texts_to_sequences([str(caption)])\n",
        "                if img_id not in token_cap_dic:\n",
        "                    token_cap_dic[img_id] = list()\n",
        "                token_cap_dic[img_id].append(cap_token)\n",
        "        return token_cap_dic"
      ],
      "metadata": {
        "id": "d9Oaedi_FrHA"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = open('/content/Flickr8k.token.txt', 'r', encoding='utf-8').read()\n",
        "description_map=dict()\n",
        "for lines in text.split('\\n'):\n",
        "  line_split = lines.split('\\t')\n",
        "  if line_split == ['']:\n",
        "      continue\n",
        "  image_id = line_split[0][:-2]\n",
        "  image_des = line_split[1]\n",
        "  #if image_id in img_dic:\n",
        "  if image_id not in description_map:\n",
        "    description_map[image_id] = list()\n",
        "  description_map[image_id].append(image_des)\n",
        "caption_list = []\n",
        "for img_id, des_list in description_map.items():\n",
        "    for i in range(len(des_list)):\n",
        "        caption = des_list[i]\n",
        "        caption = ''.join(caption)\n",
        "        caption = caption.split(' ')\n",
        "        caption = [word.lower() for word in caption if len(word) > 1 and word.isalpha()]\n",
        "        caption = ' '.join(caption)\n",
        "        des_list[i] = 'startseq ' + caption + ' endseq'\n",
        "        caption_list.append('startseq ' + caption + ' endseq')\n",
        "max_length = max(len(des.split()) for des in caption_list)\n",
        "print('max_length of captions', max_length)\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000,oov_token='<unknw>')\n",
        "tokenizer.fit_on_texts(caption_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCxkk8Ty8mpW",
        "outputId": "98d7597d-4518-40be-84c7-f6386e01ff91"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_length of captions 33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "description_map['2258277193_586949ec62.jpg'] = description_map['2258277193_586949ec62.jpg.1']\n",
        "del description_map['2258277193_586949ec62.jpg.1']"
      ],
      "metadata": {
        "id": "oCL4cB4c8vIn"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del description_map['2258277193_586949ec62.jpg']"
      ],
      "metadata": {
        "id": "-U9rQRskAzn6"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_cap_dic = dict()\n",
        "print('Vocab size',len(tokenizer.word_counts))\n",
        "for img_id, des_list in description_map.items():\n",
        "    for i in range(len(des_list)):\n",
        "        caption = des_list[i]\n",
        "        cap_token = tokenizer.texts_to_sequences([str(caption)])\n",
        "        if img_id not in token_cap_dic:\n",
        "            token_cap_dic[img_id] = list()\n",
        "        token_cap_dic[img_id].append(cap_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRgYspUC9KLe",
        "outputId": "dce2a391-0503-4910-d552-49346f7efdbc"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size 8359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reward_net_loss(visual_features,sematic_featurres,margin=0.2):\n",
        "    #bi-directional max-margin ranking loss (Bi-MMRL)\n",
        "    #visual=tf.ones(shape=(32,512))\n",
        "    #sematic=tf.ones(shape=(32,512))\n",
        "    B=visual_features.shape[0]\n",
        "    remove_diagonal_matrix=tf.ones(B)-tf.eye(B)\n",
        "    similarity_score_1=tf.einsum(\"ij,kj->ik\",visual_features,sematic_featurres)\n",
        "    similarity_score_2=tf.einsum(\"ij,kj->ki\",visual_features,sematic_featurres)\n",
        "    diagonal=tf.einsum(\"ii->i\",similarity_score_1)\n",
        "    s_ii=tf.eye(B)*tf.transpose(diagonal)\n",
        "    loss1=tf.reduce_sum(tf.maximum(0,margin+(similarity_score_1*remove_diagonal_matrix)-s_ii))\n",
        "    loss2=tf.reduce_sum(tf.maximum(0,margin+(similarity_score_2*remove_diagonal_matrix)-s_ii))\n",
        "    rn_loss=(loss1+loss2)*(1/B)\n",
        "    #birectional hard-negatives ranking loss (Bi-HNRL)\n",
        "    #loss1=tf.reduce_sum(tf.maximum(0,margin+(tf.eye(B)*tf.transpose(tf.argmax(similarity_score_1)))-s_ii))\n",
        "    #loss2=tf.reduce_sum(tf.maximum(0,margin+(tf.eye(B)*tf.transpose(tf.argmax(similarity_score_2)))-s_ii))\n",
        "    #loss=loss1+loss2\n",
        "    #loss\n",
        "    return rn_loss"
      ],
      "metadata": {
        "id": "98xuF71SBqSU"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "import numpy as np\n",
        "from data_processing import data_processing\n",
        "\n",
        "\n",
        "def load_preprocess_img(img_path):\n",
        "    img = load_img(img_path, target_size=(256, 256, 3))\n",
        "    x = img_to_array(img)\n",
        "    x /= 255.0\n",
        "    return x\n",
        "\n",
        "\n",
        "def captions_generation_reward(captions_dic, vocab_size, image_pth_rt, max_length=25, num_photos_per_batch=5, num_captions=1):\n",
        "    images, input_text_seq = list(), list()\n",
        "    batch_iter = 0\n",
        "    batch_keys = []\n",
        "    while True:\n",
        "        for key, desc_list in captions_dic.items():\n",
        "            # print(key)\n",
        "            batch_keys.append(key)\n",
        "            batch_iter += 1\n",
        "            caption = 0\n",
        "            # retrieve the photo feature\n",
        "            photo = load_preprocess_img(image_pth_rt + key)\n",
        "            for desc in desc_list:\n",
        "                caption += 1\n",
        "                desc = np.squeeze(desc)\n",
        "                input_sequence = []\n",
        "                input_seq = tf.keras.preprocessing.sequence.pad_sequences([desc], maxlen=max_length,\n",
        "                                                                          padding='pre')\n",
        "                input_text_seq.append(input_seq)\n",
        "                images.append(photo)\n",
        "                if caption == num_captions:\n",
        "                    break\n",
        "            if batch_iter == num_photos_per_batch:\n",
        "                input_text_seq = np.concatenate(input_text_seq)\n",
        "                yield [np.array(images), np.array(input_text_seq)]\n",
        "                images, input_text_seq = list(), list()\n",
        "                batch_iter = 0"
      ],
      "metadata": {
        "id": "2CAacq12PSvG"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCnr1RqfG--K",
        "outputId": "2a3d9b83-fec8-47f4-f898-33d65e173035"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version 2.7.0\n",
            "max_length of captions 33\n",
            "max_length of captions 31\n",
            "max_length of captions 30\n",
            "No of captions: Training-6000.0 Validation-1000.0 test-1000.0\n",
            "Vocab size 5000\n",
            "Vocab size 5000\n",
            "Vocab size 5000\n",
            "(5, 256, 256, 3) (5, 25)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "#from data_processing import data_processing\n",
        "#from data_generator import captions_generation\n",
        "import pickle\n",
        "\n",
        "print('TensorFlow Version', tf.__version__)\n",
        "vocab_size = 5000\n",
        "max_length = 25\n",
        "\n",
        "\n",
        "captions_text_path = r'/content/Flickr8k.token.txt'\n",
        "captions_extraction = data_processing(captions_text_path)\n",
        "trn_images_id_text = r'/content/Flickr_8k.trainImages.txt'\n",
        "train_cleaned_seq, train_cleaned_dic = captions_extraction.cleaning_sequencing_captions(trn_images_id_text)\n",
        "val_images_id_text = r'/content/Flickr_8k.devImages.txt'\n",
        "val_cleaned_seq, val_cleaned_dic = captions_extraction.cleaning_sequencing_captions(val_images_id_text)\n",
        "test_images_id_text = r'/content/Flickr_8k.testImages.txt'\n",
        "test_cleaned_seq, test_cleaned_dic = captions_extraction.cleaning_sequencing_captions(test_images_id_text)\n",
        "captions_extraction.tokenization(train_cleaned_seq, vocab_size)\n",
        "print(\"No of captions: Training-\" + str(len(train_cleaned_seq) / 5) + \" Validation-\" + str(\n",
        "    len(val_cleaned_seq) / 5) + \" test-\" + str(len(test_cleaned_seq) / 5))\n",
        "\n",
        "train_cap_tok = captions_extraction.sentence_tokenizing(train_cleaned_dic)\n",
        "val_cap_tok = captions_extraction.sentence_tokenizing(val_cleaned_dic)\n",
        "test_cap_tok = captions_extraction.sentence_tokenizing(test_cleaned_dic)\n",
        "\n",
        "image_pth_rt = r\"/content/Flicker8k_Dataset/\" #+ r\"\\\\\"\n",
        "trn_dataset = captions_generation_reward(train_cap_tok, vocab_size, image_pth_rt, max_length,num_photos_per_batch=32,num_captions=5)\n",
        "val_dataset = captions_generation_reward(val_cap_tok, vocab_size, image_pth_rt, max_length)\n",
        "\n",
        "inputs = next(iter(val_dataset))\n",
        "print(inputs[0].shape, inputs[1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trn_dataset_whole = captions_generation_reward(token_cap_dic, vocab_size, image_pth_rt, max_length,num_photos_per_batch=16,num_captions=1)"
      ],
      "metadata": {
        "id": "6x-SWJHw9Dq_"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reward_model = Caption_model_gen(NET='reward', vocab_size=5000, Embed_Size=512, max_length=max_length,display=True)\n",
        "#actor_model.summary()\n",
        "#reward_model.compile(loss=reward_net_loss,                   optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),                    metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJwJlBBfztRE",
        "outputId": "371fc98b-6b7e-437c-cd89-c67dc53db5f6"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building CNN model\n",
            "CNN model {output shape}: (None, 256)\n",
            "Building RNN model\n",
            "final_carry_state {rnn output shape}: (None, 512)\n",
            "Image feature vector shape: (None, 512)\n",
            "Text sequence vector shape: (None, 512)\n",
            "Reward Net built successfully \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cos(img_encode, cap_encode):\n",
        "    inner_product = tf.reduce_sum(tf.multiply(img_encode, cap_encode), axis=1)\n",
        "    norm1 = tf.sqrt(tf.reduce_sum(tf.square(img_encode), axis=1))\n",
        "    norm2 = tf.sqrt(tf.reduce_sum(tf.square(cap_encode), axis=1))\n",
        "    cos = inner_product / (norm1 * norm2)\n",
        "    return cos"
      ],
      "metadata": {
        "id": "0OfxwX3Swomh"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(image_encoder,caption_encoder):\n",
        "    gamma=0.2\n",
        "    N,D=image_encoder.shape\n",
        "    img_encode = image_encoder\n",
        "    cap_encode = caption_encoder\n",
        "    scores_matrix = tf.matmul(img_encode,tf.transpose(cap_encode))\n",
        "    diagonal = tf.linalg.diag_part(scores_matrix)\n",
        "    cost_cap = tf.maximum(0.0, gamma - diagonal + scores_matrix)\n",
        "    diagonal = tf.reshape(diagonal, [-1, 1])\n",
        "    cost_img = tf.maximum(0.0, gamma - diagonal + scores_matrix)\n",
        "    cost_cap = tf.linalg.set_diag(cost_cap, [0]*N)\n",
        "    cost_img = tf.linalg.set_diag(cost_img, [0]*N)\n",
        "    loss = tf.reduce_sum(cost_img) + tf.reduce_sum(cost_cap)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "jE3b26ssw6oa"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss(visuals,semantics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H6E5eDzCrQl",
        "outputId": "f746c221-1021-45fd-e3b2-49ce0a79fd34"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=396.80002>"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Rewards(reward_model,input):\n",
        "    visEmbeds, semEmbeds = reward_model(input)\n",
        "    inner_product = tf.reduce_sum(tf.multiply(visEmbeds, semEmbeds), axis=1)\n",
        "    norm1 = tf.sqrt(tf.reduce_sum(tf.square(visEmbeds), axis=1))\n",
        "    norm2 = tf.sqrt(tf.reduce_sum(tf.square(semEmbeds), axis=1))\n",
        "    cos = inner_product / (norm1 * norm2)\n",
        "    print(cos)\n",
        "    return cos"
      ],
      "metadata": {
        "id": "3ZugP37nUs8D"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reward_model1=tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/reward_net.h5')"
      ],
      "metadata": {
        "id": "PFjyFb8LU7D5"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reward_model.save('/content/drive/MyDrive/Kaggle/reward_net.h5')"
      ],
      "metadata": {
        "id": "sx-b0YUBIf3D"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "history={}\n",
        "history['loss']=list()\n",
        "history['val_loss']=list()\n",
        "metrics_names = ['loss'] \n",
        "epochs = 1\n",
        "num_epoch=200\n",
        "for i in range(epochs):\n",
        "    n=0\n",
        "    epoch=0\n",
        "    for step, x_batch_train in enumerate(trn_dataset_whole):\n",
        "        if epoch==num_epoch:\n",
        "          break\n",
        "        progbar = tf.keras.utils.Progbar(10,stateful_metrics=['loss'])\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            [visual,sematic] = reward_model(x_batch_train, training=True)  # Logits for this minibatch\n",
        "            loss_value = loss(visual, sematic)\n",
        "        grads = tape.gradient(loss_value, reward_model.trainable_weights)\n",
        "\n",
        "        optimizer.apply_gradients(zip(grads, reward_model.trainable_weights))\n",
        "        progbar.update(n, values = [(\"loss\", loss_value)])\n",
        "        # Log every 200 batches.\n",
        "        n+=1\n",
        "        if step % 10 == 0:\n",
        "            epoch+=1\n",
        "            val_ds=next(iter(val_dataset))\n",
        "            [val_visual,val_sematic] = reward_model(val_ds)\n",
        "            val_loss_value=loss(val_visual, val_sematic)\n",
        "            progbar.add(1,values = [(\"val_loss\", val_loss_value)])\n",
        "            n=0\n",
        "            print(\"\\nepoch %d\" % (epoch,))\n",
        "            history['loss'].append(loss_value)\n",
        "            history['val_loss'].append(val_loss_value)\n",
        "\n",
        "f = open(\"/content/history_reward_model_Adam_incp_lstm_3.pkl\", \"wb\")\n",
        "pickle.dump(history, f)\n",
        "f.close()   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IZuh0RCKlqYh",
        "outputId": "9a49bbb8-56c9-4528-f0c5-037b5407d598"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1/10 [==>...........................] - ETA: 10s - loss: 744.2404 - val_loss: 8.2616\n",
            "epoch 1\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 716.5579 - val_loss: 8.3891\n",
            "\n",
            "epoch 2\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 223.2908 - val_loss: 9.0825\n",
            "\n",
            "epoch 3\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 153.3571 - val_loss: 12.6480\n",
            "\n",
            "epoch 4\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 103.9655 - val_loss: 12.9546\n",
            "\n",
            "epoch 5\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 84.4280 - val_loss: 9.1547\n",
            "\n",
            "epoch 6\n",
            "10/10 [==============================] - 1s 71ms/step - loss: 70.4961 - val_loss: 13.4180\n",
            "\n",
            "epoch 7\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 75.4162 - val_loss: 13.6491\n",
            "\n",
            "epoch 8\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 80.4036 - val_loss: 19.6824\n",
            "\n",
            "epoch 9\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 71.2611 - val_loss: 29.0004\n",
            "\n",
            "epoch 10\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 56.6931 - val_loss: 14.1634\n",
            "\n",
            "epoch 11\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 97.3889 - val_loss: 21.7236\n",
            "\n",
            "epoch 12\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 53.7834 - val_loss: 6.2899\n",
            "\n",
            "epoch 13\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 82.4911 - val_loss: 22.0172\n",
            "\n",
            "epoch 14\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 59.4215 - val_loss: 21.5431\n",
            "\n",
            "epoch 15\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 81.3357 - val_loss: 35.7532\n",
            "\n",
            "epoch 16\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 78.2240 - val_loss: 51.3887\n",
            "\n",
            "epoch 17\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 66.0478 - val_loss: 33.5655\n",
            "\n",
            "epoch 18\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 61.9189 - val_loss: 19.2165\n",
            "\n",
            "epoch 19\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 52.4216 - val_loss: 41.1124\n",
            "\n",
            "epoch 20\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 55.8672 - val_loss: 27.6146\n",
            "\n",
            "epoch 21\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 65.6283 - val_loss: 18.4097\n",
            "\n",
            "epoch 22\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 73.1231 - val_loss: 14.1289\n",
            "\n",
            "epoch 23\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 71.8857 - val_loss: 10.3328\n",
            "\n",
            "epoch 24\n",
            "10/10 [==============================] - 1s 71ms/step - loss: 82.0496 - val_loss: 15.1607\n",
            "\n",
            "epoch 25\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 80.6383 - val_loss: 6.3831\n",
            "\n",
            "epoch 26\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 90.2940 - val_loss: 5.5003\n",
            "\n",
            "epoch 27\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 58.9563 - val_loss: 8.4171\n",
            "\n",
            "epoch 28\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 52.6351 - val_loss: 14.0772\n",
            "\n",
            "epoch 29\n",
            "10/10 [==============================] - 1s 71ms/step - loss: 53.1552 - val_loss: 4.9521\n",
            "\n",
            "epoch 30\n",
            "10/10 [==============================] - 1s 71ms/step - loss: 61.2927 - val_loss: 3.7129\n",
            "\n",
            "epoch 31\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 73.5300 - val_loss: 6.1142\n",
            "\n",
            "epoch 32\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 34.6793 - val_loss: 9.5573\n",
            "\n",
            "epoch 33\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 54.2413 - val_loss: 11.6776\n",
            "\n",
            "epoch 34\n",
            "10/10 [==============================] - 1s 71ms/step - loss: 44.8848 - val_loss: 8.6591\n",
            "\n",
            "epoch 35\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 43.3016 - val_loss: 7.9558\n",
            "\n",
            "epoch 36\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 46.8462 - val_loss: 5.2379\n",
            "\n",
            "epoch 37\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 40.2328 - val_loss: 12.6991\n",
            "\n",
            "epoch 38\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 38.8350 - val_loss: 6.0992\n",
            "\n",
            "epoch 39\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 44.6546 - val_loss: 26.4517\n",
            "\n",
            "epoch 40\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 46.7218 - val_loss: 4.0183\n",
            "\n",
            "epoch 41\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 59.4374 - val_loss: 10.4840\n",
            "\n",
            "epoch 42\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 42.9555 - val_loss: 3.3669\n",
            "\n",
            "epoch 43\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 55.2637 - val_loss: 7.5124\n",
            "\n",
            "epoch 44\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 64.8289 - val_loss: 8.0460\n",
            "\n",
            "epoch 45\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 39.8830 - val_loss: 7.2255\n",
            "\n",
            "epoch 46\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 38.6661 - val_loss: 14.0949\n",
            "\n",
            "epoch 47\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 39.9439 - val_loss: 10.1036\n",
            "\n",
            "epoch 48\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 37.2245"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-151-e14e0d78919d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mvisual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msematic\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Logits for this minibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msematic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    157\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1743\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m     \u001b[0mgrad_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m     \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1746\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m     \u001b[0mgrad_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   6013\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   6014\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6015\u001b[0;31m         transpose_b)\n\u001b[0m\u001b[1;32m   6016\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6017\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"/content/reward_net_whole_training_lstm_1.pkl\", \"wb\")\n",
        "pickle.dump(history, f)\n",
        "f.close()   "
      ],
      "metadata": {
        "id": "mP6E0oKE4-MJ"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = next(iter(val_dataset))\n",
        "print(inputs[0].shape, inputs[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl686AqOb2dY",
        "outputId": "a5eea4c0-e153-4565-f402-e1b87c8b2f53"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 256, 256, 3) (5, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Rewards(reward_model,[inputs[0], inputs[1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4ldPSuObxDG",
        "outputId": "d26c7c22-1687-4586-f898-5b5bfca3a60d"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 0.04418512 -0.04841695  0.00817631  0.0049956   0.05157251], shape=(5,), dtype=float32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
              "array([ 0.04418512, -0.04841695,  0.00817631,  0.0049956 ,  0.05157251],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reward_model.save('/content/drive/MyDrive/Kaggle/reward_net.h5')"
      ],
      "metadata": {
        "id": "2F96IVlyKFj9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c284f55-a9d0-4731-b589-706d91fc1323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reward_model=tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/reward_net.h5')\n",
        "#actor_model.summary()\n",
        "#reward_model.compile(loss=reward_net_loss,                   optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),                    metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4YVkrm6Knnl",
        "outputId": "9cf619eb-cbe0-43f4-ef80-08809e3d6318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = next(iter(val_dataset))\n",
        "print(inputs[0].shape, inputs[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhFihdZZ4EEH",
        "outputId": "19b31047-7236-43ac-a6b5-3dd21098153a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 256, 256, 3) (5, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r=Rewards(reward_model,[inputs[0], inputs[1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdr6mgt439pQ",
        "outputId": "7adeb6f3-263e-46dc-ccf7-81a9ed88808a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0.31575733 0.40269744 0.40483108 0.24068792 0.21883138], shape=(5,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'/content/drive/MyDrive/Kaggle/reward_net.h5'"
      ],
      "metadata": {
        "id": "-wl_u-KQKkE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2i7Ii2EWF2hd",
        "outputId": "208ed1f7-19aa-4b80-c7fc-e56006ffc95b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'val_loss'])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"/content/history_reward_model_Adam_incp_lstm_3.pkl\", \"wb\")\n",
        "pickle.dump(history, f)\n",
        "f.close() "
      ],
      "metadata": {
        "id": "sH35PugRynW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reward_model.save_weights('/content/reward_net_model_weights.h5')"
      ],
      "metadata": {
        "id": "Uxv7qvEeE9Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Rewards(model,features, captions):\n",
        "    visEmbeds, semEmbeds = model(features, captions)\n",
        "    visEmbeds=tf.norm\n",
        "    visEmbeds = F.normalize(visEmbeds, p=2, dim=1) \n",
        "    semEmbeds = F.normalize(semEmbeds, p=2, dim=1) \n",
        "    rewards = torch.sum(visEmbeds*semEmbeds, axis=1).unsqueeze(1)\n",
        "    return rewards"
      ],
      "metadata": {
        "id": "bGa4dwvB5crU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "training reward_net.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ADb8JWK0oMC1Pa0UJ7Ig4vObsTPsqxHH",
      "authorship_tag": "ABX9TyMUuJPQQi+NXtZUNfyPQAC7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}