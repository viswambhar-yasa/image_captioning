{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/viswambhar-yasa/image_captioning/blob/master/training_policy_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SCaXLacBGeyg",
    "outputId": "4d7dc7c4-ba1d-4910-8342-4b9f3da1162e"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/viswambhar-yasa/image_captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsGCW4z_Gi35"
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "def downloading_extraction(link, extraction_path='.'):\n",
    "    url = urlopen(link)\n",
    "    zipfile = ZipFile(BytesIO(url.read()))\n",
    "    zipfile.extractall(path=extraction_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    images_link = 'https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip'\n",
    "    downloading_extraction(images_link)\n",
    "    text_link = \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\"\n",
    "    downloading_extraction(text_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "907XvN7wzXz3",
    "outputId": "1c3d0e15-bcac-465c-f5fc-6cf614a47721"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version 2.8.0-rc0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Embedding, LSTM, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.applications import Xception, InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.python.keras.layers.recurrent import GRU\n",
    "\n",
    "\n",
    "def image_encoder(img_input, trainable_layers=0, CNN_Type='Xception', Embed_Size=256, display=False):\n",
    "    print('Building CNN model')\n",
    "    if CNN_Type == 'Xception':\n",
    "        cnn_pre_trained_model = Xception(include_top=False, weights='imagenet', input_tensor=img_input)\n",
    "    else:\n",
    "        cnn_pre_trained_model = InceptionV3(include_top=False, weights='imagenet', input_tensor=img_input)\n",
    "    for i, layer in enumerate(cnn_pre_trained_model.layers):\n",
    "        if len(cnn_pre_trained_model.layers) - i < trainable_layers:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "    cnn_inputs = cnn_pre_trained_model.inputs\n",
    "    base_model = cnn_pre_trained_model.output\n",
    "    base_model = GlobalAveragePooling2D(name='global_average_pooling')(base_model)\n",
    "    embed_image = tf.keras.layers.Dense(Embed_Size, activation='tanh', name='embed_image')(base_model)\n",
    "    feature_extraction_model = Model(inputs=cnn_inputs, outputs=embed_image, name='CNN encoder model')\n",
    "    print('CNN model {output shape}:', embed_image.shape)\n",
    "    if display:\n",
    "        tf.keras.utils.plot_model(feature_extraction_model, to_file='base_model.png', show_shapes=True)\n",
    "    return feature_extraction_model\n",
    "\n",
    "\n",
    "def txt_decoder(rnn_input, Embed_Size=256, Bi_Direction=False, RNN_Type='LSTM', RNN_Layers=2):\n",
    "    print('Building RNN model')\n",
    "    for i in range(RNN_Layers):\n",
    "        x = BatchNormalization()(rnn_input)\n",
    "        if RNN_Type == 'LSTM':\n",
    "            if i == (RNN_Layers - 1):\n",
    "                if Bi_Direction:\n",
    "                    rnn_out = Bidirectional(LSTM(int(Embed_Size/2)))(x)\n",
    "                else:\n",
    "                    rnn_out = LSTM(Embed_Size)(x)\n",
    "            else:\n",
    "                if Bi_Direction:\n",
    "                    rnn_out = Bidirectional(LSTM(int(Embed_Size/2), return_sequences=True))(x)\n",
    "                else:\n",
    "                    rnn_out = LSTM(Embed_Size, return_sequences=True)(x)\n",
    "        else:\n",
    "            if i == (RNN_Layers - 1):\n",
    "                if Bi_Direction:\n",
    "                    rnn_out = Bidirectional(GRU(Embed_Size))(x)\n",
    "                else:\n",
    "                    rnn_out = GRU(Embed_Size)(x)\n",
    "            else:\n",
    "                if Bi_Direction:\n",
    "                    rnn_out = Bidirectional(GRU(Embed_Size/2, return_sequences=True))(x)\n",
    "                else:\n",
    "                    rnn_out = GRU(Embed_Size, return_sequences=True)(x)\n",
    "        rnn_input = rnn_out\n",
    "    return rnn_out\n",
    "\n",
    "\n",
    "def Caption_model_gen(NET, img_shape=(256, 256, 3), vocab_size=5000, Embed_Size=256, max_length=20, display=False):\n",
    "    img_input = tf.keras.Input(shape=img_shape)\n",
    "    cnn_model = image_encoder(img_input, trainable_layers=0, CNN_Type='InceptionV3', display=False)\n",
    "    embed_image = tf.keras.layers.Dense(Embed_Size, activation='tanh')(cnn_model.output)\n",
    "\n",
    "    text_input = tf.keras.Input(shape=(max_length,))\n",
    "    Embedding_layer = Embedding(input_dim=vocab_size, output_dim=Embed_Size, input_length=max_length, mask_zero=True)(\n",
    "        text_input)\n",
    "\n",
    "    whole_seq_output = txt_decoder(Embedding_layer, Embed_Size=Embed_Size,\n",
    "                                                                          Bi_Direction=False, RNN_Type='LSTM',\n",
    "                                                                          RNN_Layers=3)\n",
    "    print('final_carry_state {rnn output shape}:', whole_seq_output.shape)\n",
    "    rnn_output = whole_seq_output\n",
    "    if NET == 'policy':\n",
    "        image_txt_embed = tf.keras.layers.add([embed_image, rnn_output])\n",
    "        print('Image and text {add shape}:', image_txt_embed.shape)\n",
    "        policy_net_output = tf.keras.layers.Dense(vocab_size, activation='softmax')(image_txt_embed)\n",
    "        policy_net_model = Model(inputs=[img_input, text_input], outputs=policy_net_output, name='Policy_Net')\n",
    "\n",
    "        print('output {shape}', policy_net_output.shape)\n",
    "        print('Policy Net built successfully \\n')\n",
    "        if display:\n",
    "            tf.keras.utils.plot_model(policy_net_model, to_file='policy_net.png', show_shapes=True)\n",
    "        return policy_net_model\n",
    "    elif NET == 'value':\n",
    "        image_txt_embed = tf.keras.layers.concatenate([embed_image, rnn_output], axis=-1)\n",
    "        print('Image and text {concat shape}:', image_txt_embed.shape)\n",
    "        hidden_layer_1 = Dense(1024, activation='tanh', name='MLP_layer1')(image_txt_embed)\n",
    "        hidden_layer_2 = Dense(512, activation='tanh', name=\"MLP_layer2\")(hidden_layer_1)\n",
    "        value_net_outputs = Dense(1, activation='tanh', name='decoder_output')(hidden_layer_2)\n",
    "        value_net_model = Model(inputs=[img_input, text_input], outputs=value_net_outputs, name='Value_Net')\n",
    "        print('output {shape}', value_net_outputs.shape)\n",
    "        print('Value Net built successfully \\n')\n",
    "        if display:\n",
    "            tf.keras.utils.plot_model(value_net_model, to_file='value_net.png', show_shapes=True)\n",
    "        return value_net_model\n",
    "    else:\n",
    "        feature_vector = Dense(512, activation='tanh')(embed_image)\n",
    "        text_sequence_vector = Dense(512, activation='tanh', name='rnn_linear')(rnn_output)\n",
    "        print('Image feature vector shape:', feature_vector.shape)\n",
    "        print('Text sequence vector shape:', text_sequence_vector.shape)\n",
    "        reward_model = Model(inputs=[img_input, text_input], outputs=[feature_vector, text_sequence_vector],\n",
    "                             name='reward net model')\n",
    "        print('Reward Net built successfully \\n')\n",
    "        if display:\n",
    "            tf.keras.utils.plot_model(reward_model, to_file='reward_net.png', show_shapes=True)\n",
    "        return reward_model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print('TensorFlow Version', tf.__version__)\n",
    "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "    #actor_model = Caption_model_gen('policy')\n",
    "    #critic_model = Caption_model_gen('value')\n",
    "    #reward = Caption_model_gen('reward')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "JNXCA7UNZXm_"
   },
   "outputs": [],
   "source": [
    "text = open('./Flickr8k.token.txt', 'r', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4qKd9U_cZcxB",
    "outputId": "4c9085d8-93b9-4ca7-b1ae-3afb9554f146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length of captions 33\n"
     ]
    }
   ],
   "source": [
    "description_map=dict()\n",
    "for lines in text.split('\\n'):\n",
    "  line_split = lines.split('\\t')\n",
    "  if line_split == ['']:\n",
    "      continue\n",
    "  image_id = line_split[0][:-2]\n",
    "  image_des = line_split[1]\n",
    "  #if image_id in img_dic:\n",
    "  if image_id not in description_map:\n",
    "    description_map[image_id] = list()\n",
    "  description_map[image_id].append(image_des)\n",
    "caption_list = []\n",
    "for img_id, des_list in description_map.items():\n",
    "    for i in range(len(des_list)):\n",
    "        caption = des_list[i]\n",
    "        caption = ''.join(caption)\n",
    "        caption = caption.split(' ')\n",
    "        caption = [word.lower() for word in caption if len(word) > 1 and word.isalpha()]\n",
    "        caption = ' '.join(caption)\n",
    "        des_list[i] = 'startseq ' + caption + ' endseq'\n",
    "        caption_list.append('startseq ' + caption + ' endseq')\n",
    "max_length = max(len(des.split()) for des in caption_list)\n",
    "print('max_length of captions', max_length)\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=1000,oov_token='<unknw>')\n",
    "tokenizer.fit_on_texts(caption_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "BQ9RQWWskJlF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8091"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "files = os.listdir(\"./Flicker8k_Dataset\")\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Y99LvKqd_iJZ"
   },
   "outputs": [],
   "source": [
    "word_index=tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "0t-RTZQQ_suO"
   },
   "outputs": [],
   "source": [
    "word_index={value:key for key, value in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tkq4FlXUAO3p",
    "outputId": "fd8befc6-486d-4a19-b279-81a5bfca78a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8360"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8092"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(description_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nmXoDBQFoTmv",
    "outputId": "b72b84fa-ea24-4717-f8d5-481776701490"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['startseq people waiting for the subway endseq',\n",
       " 'startseq some people looking out windows in large building endseq',\n",
       " 'startseq three people are waiting on train platform endseq',\n",
       " 'startseq three people standing at station endseq',\n",
       " 'startseq two woman and one man standing near train tracks endseq']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_map['2258277193_586949ec62.jpg.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "0EtHJgjkok-Q"
   },
   "outputs": [],
   "source": [
    "description_map['2258277193_586949ec62.jpg'] = description_map['2258277193_586949ec62.jpg.1']\n",
    "del description_map['2258277193_586949ec62.jpg.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "WRgzMMhL_SfS"
   },
   "outputs": [],
   "source": [
    "del description_map['2258277193_586949ec62.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "rwOe_nr6kwKr"
   },
   "outputs": [],
   "source": [
    "description_map1=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "w9h0qf3NkeNl"
   },
   "outputs": [],
   "source": [
    "files=os.listdir(\"./Flicker8k_Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "H4USEbMkkfn8"
   },
   "outputs": [],
   "source": [
    "for key,value in description_map.items():\n",
    "  if key in files:\n",
    "    description_map1[key]=value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HsG1nZclawvt",
    "outputId": "4c788184-d84e-4f93-e596-3c13170aca22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8359\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8091\n"
     ]
    }
   ],
   "source": [
    "print(len(description_map1.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10694/4061971893.py:9: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  description_map_subset = dict(sample(description_map1.items(),num_images))\n"
     ]
    }
   ],
   "source": [
    "from os import path, mkdir\n",
    "from random import sample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas import Series\n",
    "num_images = 4000\n",
    "if not path.exists('./subsets'):\n",
    "    mkdir('./subsets')\n",
    "if(num_images != len(description_map1.keys())):\n",
    "    description_map_subset = dict(sample(description_map1.items(),num_images))\n",
    "    train_images_id, test_images_id = train_test_split(Series(description_map_subset.keys()),test_size=0.125,random_state=8)\n",
    "    train_images_id, val_imgs_id = train_test_split(Series(description_map_subset.keys()),test_size=0.125, random_state=8)\n",
    "    train_images_id.to_csv('./subsets/Flickr8k_images_train.txt',sep=' ',index=False,header=False)\n",
    "    test_images_id.to_csv('./subsets/Flickr8k_images_test.txt',sep=' ',index=False,header=False)\n",
    "    val_imgs_id.to_csv('./subsets/Flickr8k_images_val.txt',sep=' ',index=False,header=False)\n",
    "else:\n",
    "    description_map_subset = description_map1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bXtMy9peZprb",
    "outputId": "4c440982-044b-4633-84e2-676516ca62f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 8359\n"
     ]
    }
   ],
   "source": [
    "token_cap_dic = dict()\n",
    "print('Vocab size',len(tokenizer.word_counts))\n",
    "for img_id, des_list in description_map_subset.items():\n",
    "    for i in range(len(des_list)):\n",
    "        caption = des_list[i]\n",
    "        cap_token = tokenizer.texts_to_sequences([str(caption)])\n",
    "        if img_id not in token_cap_dic:\n",
    "            token_cap_dic[img_id] = list()\n",
    "        token_cap_dic[img_id].append(cap_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "d9Oaedi_FrHA"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class data_processing:\n",
    "    def __init__(self, text_file_path):\n",
    "        self.text_file_path = text_file_path\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def extraction_captions(self, images_id_text):\n",
    "        description_map = dict()\n",
    "        text = open(self.text_file_path, 'r', encoding='utf-8').read()\n",
    "        images = open(images_id_text, 'r', encoding='utf-8').read()\n",
    "        img_dic = []\n",
    "        for img_id in images.split('\\n'):\n",
    "            img_dic.append(img_id)\n",
    "        for lines in text.split('\\n'):\n",
    "            line_split = lines.split('\\t')\n",
    "            if line_split == ['']:\n",
    "                continue\n",
    "            image_id = line_split[0][:-2]\n",
    "            image_des = line_split[1]\n",
    "            if image_id in img_dic:\n",
    "                if image_id not in description_map:\n",
    "                    description_map[image_id] = list()\n",
    "                description_map[image_id].append(image_des)\n",
    "        return description_map\n",
    "\n",
    "    def cleaning_sequencing_captions(self, images_id_text):\n",
    "        captions_dic = self.extraction_captions(images_id_text)\n",
    "        caption_list = []\n",
    "        for img_id, des_list in captions_dic.items():\n",
    "            for i in range(len(des_list)):\n",
    "                caption = des_list[i]\n",
    "                caption = ''.join(caption)\n",
    "                caption = caption.split(' ')\n",
    "                caption = [word.lower() for word in caption if len(word) > 1 and word.isalpha()]\n",
    "                caption = ' '.join(caption)\n",
    "                des_list[i] = 'startseq ' + caption + ' endseq'\n",
    "                caption_list.append('startseq ' + caption + ' endseq')\n",
    "        max_length = max(len(des.split()) for des in caption_list)\n",
    "        print('max_length of captions', max_length)\n",
    "        return caption_list,captions_dic\n",
    "\n",
    "    def tokenization(self, captions_for_token, num_wrds=5000) -> None:\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_wrds, oov_token='<unknw>')\n",
    "        tokenizer.fit_on_texts(captions_for_token)\n",
    "        self.tokenizer = tokenizer\n",
    "        return tokenizer\n",
    "\n",
    "    def sentence_tokenizing(self, captions_dic) -> dict:\n",
    "        token_cap_dic = dict()\n",
    "        print('Vocab size', self.tokenizer.num_words)\n",
    "        for img_id, des_list in captions_dic.items():\n",
    "            for i in range(len(des_list)):\n",
    "                caption = des_list[i]\n",
    "                cap_token = self.tokenizer.texts_to_sequences([str(caption)])\n",
    "                if img_id not in token_cap_dic:\n",
    "                    token_cap_dic[img_id] = list()\n",
    "                token_cap_dic[img_id].append(cap_token)\n",
    "        return token_cap_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "2CAacq12PSvG"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "import numpy as np\n",
    "from data_processing import data_processing\n",
    "\n",
    "\n",
    "def load_preprocess_img(img_path):\n",
    "    img = load_img(img_path, target_size=(256, 256, 3))\n",
    "    x = img_to_array(img)\n",
    "    x /= 255.0\n",
    "    return x\n",
    "\n",
    "\n",
    "def captions_generation(captions_dic, vocab_size, image_pth_rt, max_length=25, num_photos_per_batch=5, num_captions=1):\n",
    "    images, input_text_seq, output_text = list(), list(), list()\n",
    "    batch_iter = 0\n",
    "    batch_keys = []\n",
    "    while True:\n",
    "        for key, desc_list in captions_dic.items():\n",
    "            # print(key)\n",
    "            batch_keys.append(key)\n",
    "            batch_iter += 1\n",
    "            caption = 0\n",
    "            # retrieve the photo feature\n",
    "\n",
    "            photo = load_preprocess_img(image_pth_rt + key)\n",
    "            \n",
    "            for desc in desc_list:\n",
    "                caption += 1\n",
    "                desc = np.squeeze(desc)\n",
    "                input_sequence = []\n",
    "                out_text=[]\n",
    "                for i in range(0, len(desc)-1):\n",
    "                    input_sequence.append(desc[:i ])\n",
    "                    out_text.append(desc[i+1])\n",
    "                    images.append(photo)\n",
    "                \n",
    "                input_seq = tf.keras.preprocessing.sequence.pad_sequences(input_sequence, maxlen=max_length,\n",
    "                                                                          padding='post')\n",
    "                #input_text = input_seq[:, :-1]\n",
    "                #out_text = input_seq[:, -1]\n",
    "                output_sequence = tf.keras.utils.to_categorical(out_text, num_classes=vocab_size)\n",
    "                input_text_seq.append(input_seq)\n",
    "                output_text.append(output_sequence)\n",
    "                if caption == num_captions:\n",
    "                    break\n",
    "            if batch_iter == num_photos_per_batch:\n",
    "                input_text_seq = np.concatenate(input_text_seq)\n",
    "                output_text = np.concatenate(output_text)\n",
    "                #print(batch_keys[-5:])\n",
    "                yield [[np.array(images), np.array(input_text_seq)], np.array(output_text)]\n",
    "                images, input_text_seq, output_text = list(), list(), list()\n",
    "                batch_iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "9fkiF7ENolW2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GCnr1RqfG--K",
    "outputId": "f91969e1-c4ef-454e-c70e-335a76252df5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version 2.8.0-rc0\n",
      "max_length of captions 33\n",
      "max_length of captions 27\n",
      "max_length of captions 27\n",
      "No of captions: Training-3500.0 Validation-500.0 test-500.0\n",
      "Vocab size 1000\n",
      "Vocab size 1000\n",
      "Vocab size 1000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#from data_processing import data_processing\n",
    "#from data_generator import captions_generation\n",
    "import pickle\n",
    "\n",
    "print('TensorFlow Version', tf.__version__)\n",
    "vocab_size = 1000\n",
    "max_length = 10\n",
    "\n",
    "\n",
    "captions_text_path = './Flickr8k.token.txt'\n",
    "captions_extraction = data_processing(captions_text_path)\n",
    "trn_images_id_text = r'./subsets/Flickr8k_images_train.txt'\n",
    "train_cleaned_seq, train_cleaned_dic = captions_extraction.cleaning_sequencing_captions(trn_images_id_text)\n",
    "val_images_id_text = r'./subsets/Flickr8k_images_val.txt'\n",
    "val_cleaned_seq, val_cleaned_dic = captions_extraction.cleaning_sequencing_captions(val_images_id_text)\n",
    "test_images_id_text = r'./subsets/Flickr8k_images_test.txt'\n",
    "test_cleaned_seq, test_cleaned_dic = captions_extraction.cleaning_sequencing_captions(test_images_id_text)\n",
    "tokenizer=captions_extraction.tokenization(train_cleaned_seq, vocab_size)\n",
    "print(\"No of captions: Training-\" + str(len(train_cleaned_seq) / 5) + \" Validation-\" + str(\n",
    "    len(val_cleaned_seq) / 5) + \" test-\" + str(len(test_cleaned_seq) / 5))\n",
    "\n",
    "train_cap_tok = captions_extraction.sentence_tokenizing(train_cleaned_dic)\n",
    "val_cap_tok = captions_extraction.sentence_tokenizing(val_cleaned_dic)\n",
    "test_cap_tok = captions_extraction.sentence_tokenizing(test_cleaned_dic)\n",
    "\n",
    "image_pth_rt = r\"./Flicker8k_Dataset/\" #+ r\"\\\\\"\n",
    "trn_dataset = captions_generation(train_cap_tok, vocab_size, image_pth_rt, max_length,5,1)\n",
    "val_dataset = captions_generation(val_cap_tok, vocab_size, image_pth_rt, max_length)\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "wadAX3Yqb2g1"
   },
   "outputs": [],
   "source": [
    "trn_dataset_whole = captions_generation(token_cap_dic, 1000, image_pth_rt, max_length,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mv_K1blfZEYH",
    "outputId": "c2993f3b-e408-4a85-87e8-c16697d414e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17500"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_cleaned_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v0v_E-hekGpx",
    "outputId": "8a7709fc-6da5-43a9-c566-5e72c7ed4e5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, 256, 256, 3) (39, 10) (39, 1000)\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "inputs, outputs = next(iter(trn_dataset_whole))\n",
    "print(inputs[0].shape, inputs[1].shape, outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ublTG7UVcwAG",
    "outputId": "acf8804d-b934-4e54-d8fe-803a1541484e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hJwJlBBfztRE",
    "outputId": "befe2e25-0853-4cc5-bc85-5879e92acf04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building CNN model\n",
      "CNN model {output shape}: (None, 256)\n",
      "Building RNN model\n",
      "final_carry_state {rnn output shape}: (None, 256)\n",
      "Image and text {add shape}: (None, 256)\n",
      "output {shape} (None, 1000)\n",
      "Policy Net built successfully \n",
      "\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "actor_model = Caption_model_gen(NET='policy', vocab_size=1000, Embed_Size=256, max_length=max_length,display=True)\n",
    "#actor_model.summary()\n",
    "actor_model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZzLzRr37zrn5",
    "outputId": "d8292689-d70c-4c8b-9ea8-8a23bffeaccd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 17s 562ms/step - loss: 6.2362 - accuracy: 0.1032 - val_loss: 5.8541 - val_accuracy: 0.1064\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 5s 404ms/step - loss: 5.2723 - accuracy: 0.1363 - val_loss: 5.3019 - val_accuracy: 0.0702\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 3s 242ms/step - loss: 5.3112 - accuracy: 0.1117 - val_loss: 5.3608 - val_accuracy: 0.1111\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 3s 238ms/step - loss: 4.9563 - accuracy: 0.1414 - val_loss: 5.9360 - val_accuracy: 0.0556\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 3s 248ms/step - loss: 5.0683 - accuracy: 0.1374 - val_loss: 5.3467 - val_accuracy: 0.0698\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 3s 231ms/step - loss: 5.2793 - accuracy: 0.1348 - val_loss: 4.7628 - val_accuracy: 0.0732\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 3s 267ms/step - loss: 4.9841 - accuracy: 0.1344 - val_loss: 5.2926 - val_accuracy: 0.0508\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 3s 271ms/step - loss: 5.1193 - accuracy: 0.1364 - val_loss: 5.3461 - val_accuracy: 0.0741\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 5s 408ms/step - loss: 5.0122 - accuracy: 0.1444 - val_loss: 5.1965 - val_accuracy: 0.0645\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 4s 300ms/step - loss: 4.9548 - accuracy: 0.1478 - val_loss: 5.3902 - val_accuracy: 0.0755\n",
      "Epoch 11/100\n",
      " 9/12 [=====================>........] - ETA: 0s - loss: 5.1005 - accuracy: 0.1259"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [73]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m model_checkpoint_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[1;32m     15\u001b[0m     filepath\u001b[38;5;241m=\u001b[39mcheckpoint_filepath,\n\u001b[1;32m     16\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m callback \u001b[38;5;241m=\u001b[39m [model_checkpoint_callback]\n\u001b[0;32m---> 21\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mactor_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrn_dataset_whole\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#history = actor_model.fit(trn_dataset, steps_per_epoch=10, epochs=100, shuffle=False,callbacks=lr_callback)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m model_parameters \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/envs/captioning-3.10.1/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/envs/captioning-3.10.1/lib/python3.10/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/envs/captioning-3.10.1/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/envs/captioning-3.10.1/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/envs/captioning-3.10.1/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/envs/captioning-3.10.1/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/envs/captioning-3.10.1/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/envs/captioning-3.10.1/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/envs/captioning-3.10.1/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "checkpoint_filepath = '/content'\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', patience=10)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='accuracy',\n",
    "    mode='auto')\n",
    "\n",
    "callback = [model_checkpoint_callback]\n",
    "\n",
    "history = actor_model.fit(trn_dataset_whole,epochs=100,steps_per_epoch=12, shuffle=False, validation_data=val_dataset,validation_steps=1)\n",
    "#history = actor_model.fit(trn_dataset, steps_per_epoch=10, epochs=100, shuffle=False,callbacks=lr_callback)\n",
    "model_parameters = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQFFrV4IduFF"
   },
   "outputs": [],
   "source": [
    "f = open(\"./output/history_policy_model_lstm_3.pkl\", \"wb\")\n",
    "pickle.dump(model_parameters, f)\n",
    "f.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkdzs3PaiWj9"
   },
   "outputs": [],
   "source": [
    "actor_model.save_weights('./output/policy_net_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRe7oA5HiyWl"
   },
   "outputs": [],
   "source": [
    "actor_model.load_weights('./output/policy_net_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJMSCR66D6Hf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJLIxu97Dzs_"
   },
   "outputs": [],
   "source": [
    "weights = tf.Variable(actor_model.layers[-23].get_weights()[0][1:])\n",
    "# Create a checkpoint from embedding, the filename and key are the\n",
    "# name of the tensor.\n",
    "log_dir=r'./output/sample_data'\n",
    "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
    "# Set up config.\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`.\n",
    "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "embedding.metadata_path = 'metadata.tsv'\n",
    "projector.visualize_embeddings(log_dir, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "or43YYkvC2AE",
    "outputId": "5f72b4e4-cc93-450e-c3c7-800fbb108102"
   },
   "outputs": [],
   "source": [
    "actor_model.layers[-23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fnc42s_bEQKq",
    "outputId": "1594a67d-02d0-4469-da93-352417db5086"
   },
   "outputs": [],
   "source": [
    "# Now run tensorboard against on log data we just saved.\n",
    "#!tensorboard --logdir /logs/\n",
    "from collections.abc import Mapping\n",
    "!tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "xndtNRnFkeoE",
    "outputId": "82fcd1ab-3b98-428d-a5d5-678b99a3e283"
   },
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "f = open(\"./output/history_policy_model_lstm_3.pkl\", \"wb\")\n",
    "pickle.dump(model_parameters, f)\n",
    "f.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sj9y4cXs-gRv"
   },
   "outputs": [],
   "source": [
    "   import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "3eMm-zoHAQfW",
    "outputId": "806d9917-91a7-47c3-dacf-e4ce7344aa7b"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label=\"loss\")\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "7aNbVQcUMj5O",
    "outputId": "d1e47769-e107-4333-85fd-367e1422b59d"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label=\"accuracy\")\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "id": "3YtPObwH-Vgq",
    "outputId": "587cb6d9-e826-42ae-a79b-66b7d5148f19"
   },
   "outputs": [],
   "source": [
    "plt.semilogx(history.history[\"lr\"],history.history['loss'])\n",
    "plt.axis([0.001,0.0001,4.5,6.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNMRkq7mKtmZ"
   },
   "outputs": [],
   "source": [
    "actor_model.save_weights('./output/policy_net_model_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7uzwYUotGOvW"
   },
   "outputs": [],
   "source": [
    "#actor_model.load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnrxXXoQ-15m"
   },
   "outputs": [],
   "source": [
    "def caption_greedy(policy_net,image,tokenizer,word_index,max_length=20):\n",
    "  caption='startseq'\n",
    "  for i in range(max_length):\n",
    "    input_seq=caption.split(' ')\n",
    "    tokenization=tokenizer.texts_to_sequences(input_seq)\n",
    "    padding=tf.keras.preprocessing.sequence.pad_sequences([tokenization],maxlen=max_length) \n",
    "    predicted_word_index=np.argmax(policy_net.predict([tf.expand_dims(image,axis=0),padding]))\n",
    "    predicted_word=word_index[predicted_word_index]\n",
    "    caption+=' '+predicted_word\n",
    "    if predicted_word =='endseq':\n",
    "      break\n",
    "  return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpg8naTN3LzL"
   },
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=3000,oov_token='<unknw>')\n",
    "tokenizer.fit_on_texts(caption_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "B9FAQiioYOEi",
    "outputId": "a1926f4c-6d67-43d9-8258-4733e956854d"
   },
   "outputs": [],
   "source": [
    "test_img=list(test_cap_tok.keys())[250]\n",
    "test_photo = load_preprocess_img(image_pth_rt + test_img)\n",
    "cap=caption_greedy(actor_model,test_photo,tokenizer,word_index,max_length=max_length)\n",
    "plt.imshow(test_photo)\n",
    "plt.title(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oijeDQqNftdq"
   },
   "outputs": [],
   "source": [
    "test_dataset = captions_generation(test_cap_tok, vocab_size, image_pth_rt, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmAr0a6uE8n_"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def captions_eval(captions_dic, vocab_size, image_pth_rt,tokenizer, max_length=25, num_captions=1):\n",
    "    images, input_text_seq, output_text = list(), list(), list()\n",
    "    batch_iter = 0\n",
    "    batch_keys = []\n",
    "    while True:\n",
    "        for key, desc_list in captions_dic.items():\n",
    "            # print(key)\n",
    "            batch_keys.append(key)\n",
    "            batch_iter += 1\n",
    "            caption = 0\n",
    "            # retrieve the photo feature\n",
    "\n",
    "            photo = load_preprocess_img(image_pth_rt + key)\n",
    "            for desc in desc_list:\n",
    "                caption += 1\n",
    "                desc = np.squeeze(desc)\n",
    "                input_sequence = []\n",
    "                for i in range(1, len(desc)):\n",
    "                    input_sequence.append(desc[:i + 1])\n",
    "                    images.append(photo)\n",
    "                input_seq = tf.keras.preprocessing.sequence.pad_sequences(input_sequence, maxlen=max_length,\n",
    "                                                                          padding='pre')\n",
    "                if caption == num_captions:\n",
    "                    break\n",
    "                #predicted_cap=caption_greedy(policy_net,photo,tokenizer)\n",
    "                #BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], predicted_cap, weights = [1])\n",
    "                #print(BLEUscore)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "GEOFx-UfKsw9",
    "outputId": "3f6fb1bb-c5f4-4a30-b93c-afc0c1f51f80"
   },
   "outputs": [],
   "source": [
    "actor_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyM7Zk8/5vbS6l6V1MfWnICW",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "training_policy_net.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
